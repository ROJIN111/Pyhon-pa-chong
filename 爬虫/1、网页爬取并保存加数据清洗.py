#-"-coding:utf-8-"-
import xlwt
import requests
from bs4 import BeautifulSoup
from lxml import etree
# çˆ¬å–ç½‘é¡µå†…å®¹
import requests
import re
import os
from tqdm import tqdm


#é€šå¸¸åªé’ˆå¯¹æ–‡å­—ï¼Œè€Œç¬¦å·ã€æ•°å­—ç­‰æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼Œå¦‚æœä¸€é¡¹é¡¹çš„åˆ†å¼€å»é™¤ï¼Œé‚£æ ·å°±ä¼šæµªè´¹æ—¶é—´ï¼Œæ‰€ä»¥æƒ³åªç•™ä¸‹æ±‰å­æ—¶ï¼Œ
def is_chinese(uchar):
    if uchar >= u'\u4e00' and uchar <= u'\u9fa5':  # åˆ¤æ–­ä¸€ä¸ªucharæ˜¯å¦æ˜¯æ±‰å­—
        return True
    else:
        return False

def allcontents(contents):
    content = ''
    for i in contents:
        if is_chinese(i):
            content = content + i
    print('\n' + content)
    return content




# è¾“å…¥ç½‘é¡µurl
for p in range(0,400,20):
    url = 'https://movie.douban.com/subject/35875180/reviews?start=' + str(p)
    # .è®¾ç½®headersç›®çš„
    # åœ¨è¯·æ±‚ç½‘é¡µçˆ¬å–çš„æ—¶å€™è¾“å‡ºçš„textä¿¡æ¯ä¸­ä¼šå‡ºç°æŠ±æ­‰ï¼Œæ— æ³•è®¿é—®ç­‰å­—çœ¼ï¼Œè¿™å°±æ˜¯ç½‘é¡µè®¾ç½®äº†ç¦æ­¢çˆ¬å–ã€‚
    # headersæ˜¯è§£å†³requestsè¯·æ±‚åçˆ¬çš„æ–¹æ³•ä¹‹ä¸€ï¼Œç›¸å½“äºæˆ‘ä»¬è¿›å»è¿™ä¸ªç½‘é¡µçš„æœåŠ¡å™¨æœ¬èº«ï¼Œå‡è£…è‡ªå·±æœ¬èº«åœ¨çˆ¬å–æ•°æ®
    headers = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'Accept - Encoding': "gzip, deflate",
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
        'Connection': 'keep-alive',
        'Host': 'movie.douban.com',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0'}
    # è¾“å…¥è¦çˆ¬å–çš„ç½‘ç«™
    resource = requests.get(url, headers=headers)
    resource.encoding = "utf-8"
    r = resource
    text = resource.content.decode("utf-8", "ignore")
    text = r.text
    tree = etree.HTML(text)
    items = tree.xpath('/html/body/div[3]/div[1]/div/div[1]/div[1]')
    # aa = []
    # w = 0
    comments_count = 0
    for item in items:
        for i in range(1, 20, 1):
            if i <= 21:
                # name = item.xpath("/html/body/div[3]/div[1]/div/div[1]/div[1]/div[" + str(i) + "]/div/header/a[2]/text()")[0]
                # clear_character(name)

                # Raca = item.xpath("/html/body/div[3]/div[1]/div/div[1]/div[1]/div[" + str(i) + "]/div/header/span[1]")

                comments = item.xpath(
                    "/html/body/div[3]/div[1]/div/div[1]/div[1]/div[" + str(i) + "]/div/div/div[1]/div/text()")
                comments = str(comments)

                b=allcontents(comments)

                # f = open("sentiment_comments.txt", "r")
                # lines = f.readlines()  # è¯»å–å…¨éƒ¨å†…å®¹ ï¼Œå¹¶ä»¥åˆ—è¡¨æ–¹å¼è¿”å›
                # for line in lines:
                #     print (line)
                # # -*-coding:utf8-*-# encoding:utf-8
                # name_list = b
                # for i in tqdm(name_list):
                #     f = open('D:/project/tm_caption/file_names.txt', 'a')
                #     f.write(str(i) + '\n')
                #     f.close()
                # sp = open("D:\pythonProject\STUDY\YUYAN\çˆ¬è™«\sentiment_comments.txt", "a", encoding='utf8')
                # sp.writelines(b)
                with open('/YUYAN/çˆ¬è™«\sentiment_comments.txt', 'a') as f:
                    f.write(b+'\n')





            if i > 21:
                print('å‡ºé”™')

print("è¾“å‡ºé¡µæ•°{}".format(i))
print('å®Œæˆ')

            # aa.append(name)
            # aa.append(clear_character(comments))
            # Name = item.xpath("/html/body/div[3]/div[1]/div/div[1]/div[1]/div["+str(i)+"]/div/header/span[1]")
            # ent_text = etree.tostring(Name[0], method='text', encoding='utf-8')
            # print(ent_text.decode())

            # soup = BeautifulSoup('https://movie.douban.com/subject/35875180/reviews?start=', 'html.parser')
            # s1 = soup.find('span', attrs={"class": "red"})  # æŸ¥æ‰¾span classä¸ºredçš„å­—ç¬¦ä¸²
            # s2 = soup.find_all("span")  # æŸ¥æ‰¾æ‰€æœ‰çš„span
            # result = [span.get_text() for span in s2]
            # print(result)  # ['item1', 'item2']
            # Note = open('å½±è¯„.txt', mode='w')
            # Note.writelines(n)  # \n æ¢è¡Œç¬¦




# def clear_character(r):
#     pattern = re.compile("['[\[\]\s+\.\!\/_,$%^*(+\"\'?:&@#;<>=-]+|[+\
#           â€”â€”ï¼ï¼Ÿ~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰ã€ã€Œâ€™]+|[ã€Šã€‹ ï¼Œã€‚ï¼›ï¹”ã€ï¼š\- â˜… ã€ ã€‘ ï¼ ğŸ˜‚ â€œ â€ ğŸ™ƒ ğŸ˜… â€¢]+|[a-zA-Z]+|[0-9]+', '']")  # åªä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—å’Œç¬¦å·ï¼Œå»æ‰å…¶ä»–ä¸œè¥¿
#     # è‹¥åªä¿ç•™ä¸­è‹±æ–‡å’Œæ•°å­—ï¼Œåˆ™æ›¿æ¢ä¸º[^\u4e00-\u9fa5^a-z^A-Z^0-9]
#     line = re.sub(pattern, '',r )  # æŠŠæ–‡æœ¬ä¸­åŒ¹é…åˆ°çš„å­—ç¬¦æ›¿æ¢æˆç©ºå­—ç¬¦
#     new_sentence = ''.join(line.split())  # å»é™¤ç©ºç™½
#     print('',new_sentence)
#     return pattern ,line,new_sentence



#
#
# #åŒ¹é…æ•°å­—
# def find_chinese(text):
#     pattern=re.compile(r'[\u4e00-\u9fa5]')
#     chinese_txt=re.sub(pattern,'',text)
#     return chinese_txt








# def delelem(data):
#     """
#     åˆ é™¤å¤šä½™çš„æ ‡ç‚¹ç¬¦å·   å‡ºç°çš„æ— ç”¨ç©ºæ ¼   ä¸å‡ºç°ä¸­æ–‡å­—ç¬¦çš„æ•°æ®è¡Œ
#     :param data: list
#     :return: list
#     """
#     res_del = []
#
#     for i in data:
#         # ä½¿ç”¨ç©ºå­—ç¬¦æ›¿æ¢æ‰é—´éš”ç¬¦
#         a = re.sub(r'\s', '', i)
#
#         # ä½¿ç”¨ç²¾å‡†åŒ¹é…ï¼ŒåŒ¹é…è¿ç»­å‡ºç°çš„ç¬¦å·;å¹¶ç”¨ç©ºå­—ç¬¦æ›¿æ¢ä»–
#         b = re.sub(r'\W{2,}', '', a)
#
#         # ä½¿ç”¨ç©ºå­—ç¬¦æ›¿æ¢ç©ºæ ¼
#         c = re.sub(r' ', '', b)
#
#         # åˆ é™¤æ²¡æœ‰ä¸­æ–‡çš„æ•°æ®è¡Œ
#         if len(re.findall(r"[\u4e00-\u9fa5]", c)) >= 10:
#             res_del.append(c)
#
#     return res_del
